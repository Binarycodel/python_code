{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-02896e0781f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "  dir(nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = stopwords.words(\"english\")\n",
    "print('stopword = ', w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = \"this is the greatest thing ever\"\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZE EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READING TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = \"\"\n",
    "with open(\"SMSSPamCollection\") as dataset:\n",
    "    ds = dataset.read()\n",
    "    print(ds[0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.replace('\\t', \"\\n\").split(\"\\n\")\n",
    "print(ds[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ds[0::2]\n",
    "msg_list = ds[1::2]\n",
    "print(label_list[0:5])\n",
    "print(msg_list[0:5])\n",
    "print(len(label_list))\n",
    "print(len(msg_list))\n",
    "print(label_list[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORGANIZING THE DATASET USING PANDAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a996a2be5080>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m data_frame = pd.DataFrame({\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;34m\"label\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlabel_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;34m\"message\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmsg_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m })\n",
      "\u001b[1;31mNameError\u001b[0m: name 'label_list' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "\n",
    "data_frame = pd.DataFrame({\n",
    "    \"label\":label_list[:-1],\n",
    "    \"message\":msg_list\n",
    "})\n",
    "\n",
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2 of reading data using panda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"SMSSPamCollection\" , sep=\"\\t\" , header=None)\n",
    "data.columns = [\"label\" , \"message\"]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row 5572 , columes 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"row {len(data)} , columes {len(data.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " label = data[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham count 4825\n",
      "spam count 747\n",
      "Index(['label', 'message'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "ham_count = len([h for h in data[\"label\"] if h==\"ham\"])\n",
    "print (\"ham count\" , ham_num)\n",
    "spam_count = len([h for h in data[\"label\"] if h==\"spam\"])\n",
    "print (\"spam count\", spam_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualizaing spam and ham on Kaggle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot as pylt\n",
    "import numpy as np \n",
    "X = np.array(['Ham' , 'Spam'])\n",
    "y = np.array(ham_num , spam_count)\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'ham or missing data = {data['lebel'].isnull().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('missing ham value' , data['label'].isnull().sum())\n",
    "print('missing spam value' , data['label'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "pd.set_option('display.max_colwidth' , 100)\n",
    "dataset = pd.read_csv('SMSSpamCollection', sep='\\t' , header=None)\n",
    "dataset.columns = ['label', 'message']\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    result = ''.join([c for c in text if c not in string.punctuation])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['cln_msg'] = [remove_punctuation(m) for m in dataset['message']]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['token_msg'] = [word_tokenize(dp) for dp in dataset['cln_msg']]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# romove stop word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "st = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword(word):\n",
    "    clean_word = [w for w in word if w not in st]\n",
    "    return clean_word\n",
    "\n",
    "dataset['token_msg'] = dataset['token_msg'].apply(lambda w: remove_stopword(w))\n",
    "dataset.head()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stemming the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn_stem = WordNetLemmatizer()\n",
    "\n",
    "def stem_word(word):\n",
    "    return [wn_stem.lemmatize(w) for w in word]\n",
    "\n",
    "dataset['stem_msg'] = dataset['token_msg'].apply(lambda x: stem_word(x))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vectorizing the corpus (count vectorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'this is a sentence is',\n",
    "    'this is another sentence',\n",
    "    'third document is here'\n",
    "]\n",
    "\n",
    "\n",
    "X = cv.fit(corpus)\n",
    "print(X.vocabulary_)\n",
    "print(cv.get_feature_names())\n",
    "\n",
    "X = cv.fit_transform(corpus)\n",
    "print(X.shape)\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pd.DataFrame(X.toarray() , columns= cv.get_feature_names())\n",
    "p.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load datase using pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords as st\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                               message  \n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...  \n",
       "1                                                                        Ok lar... Joking wif u oni...  \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n",
       "3                                                    U dun say so early hor... U c already then say...  \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth' , 100)\n",
    "dataset = pd.read_csv('SMSSpamCollection' , sep='\\t' , header=None)\n",
    "dataset.columns = ['label', 'message']\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data preprocessing using custom method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>cln_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "      <td>[Go, jurong, point, crazi, avail, bugi, n, great, world, la, e, buffet, cine, got, amor, wat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>[Ok, lar, joke, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, FA, cup, final, tkt, 21st, may, 2005, text, FA, 87121, receiv,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>[U, dun, say, earli, hor, U, c, alreadi, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>[nah, I, dont, think, goe, usf, live, around, though]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                               message  \\\n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...   \n",
       "1                                                                        Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "3                                                    U dun say so early hor... U c already then say...   \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "\n",
       "                                                                                           cln_message  \n",
       "0        [Go, jurong, point, crazi, avail, bugi, n, great, world, la, e, buffet, cine, got, amor, wat]  \n",
       "1                                                                         [Ok, lar, joke, wif, u, oni]  \n",
       "2  [free, entri, 2, wkli, comp, win, FA, cup, final, tkt, 21st, may, 2005, text, FA, 87121, receiv,...  \n",
       "3                                                        [U, dun, say, earli, hor, U, c, alreadi, say]  \n",
       "4                                                [nah, I, dont, think, goe, usf, live, around, though]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(string.punctuation)\n",
    "stop_word = st.words('english')\n",
    "lemma = WordNetLemmatizer()\n",
    "stemma = PorterStemmer()\n",
    "#print(lemma.lemmatize('cacti'))\n",
    "\n",
    "def clean_dataset(text):\n",
    "    data = ''.join([t for t in text if t not in string.punctuation])\n",
    "    token = re.split('\\W+', data)\n",
    "    data = [stemma.stem(t) for t in token if t not in stop_word]\n",
    "    return data\n",
    "\n",
    "dataset['cln_message'] = dataset['message'].apply(lambda x: clean_dataset(x))\n",
    "dataset.head()\n",
    "#print(dataset['cln_message'][0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting dataset into training and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X lenght 4179\n",
      "x text len  1393\n",
      "y train lenght 4179\n",
      "y test len 1393\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(dataset['message'], dataset['label'], test_size=0.25)\n",
    "print('X lenght' , len(X_train))\n",
    "print('x text len ', len(X_test))\n",
    "print('y train lenght', len(y_train))\n",
    "print('y test len', len(y_test))\n",
    "print('hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extracting feature from text using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1393x5000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 12180 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "cv = CountVectorizer(analyzer=clean_dataset, max_features=5000)\n",
    "\n",
    "cv.fit(X_train)\n",
    "X_train_cv = cv.transform(X_train)\n",
    "\n",
    "\n",
    "X_test_cv  = cv.transform(X_test)\n",
    "\n",
    "X_train_cv\n",
    "X_test_cv\n",
    "\n",
    "#print(X.vocabulary_)\n",
    "#print(cv.get_feature_names())\n",
    "\n",
    "# encoding label ham =0 , spam = 1 thus binary encoding\n",
    "# label_encoder = preprocessing.LabelEncoder()\n",
    "# y = label_encoder.fit_transform(dataset['label'])\n",
    "# print(y[:10])\n",
    "\n",
    "# print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usign just 10 row of our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini_dataset = dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv2 = CountVectorizer(analyzer=clean_dataset)\n",
    "# X = cv2.fit_transform(mini_dataset['message'])\n",
    "# print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe = pd.DataFrame(X.toarray() , columns = cv2.get_feature_names())\n",
    "# dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive bayes Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes, metrics, svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = naive_bayes.MultinomialNB(alpha=0.2)\n",
    "mnb.fit(X_train_cv , y_train)\n",
    "\n",
    "y_mnb = mnb.predict(X_test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy 0.9863603732950467\n",
      "confustion matric  [[1205   14]\n",
      " [   5  169]]\n"
     ]
    }
   ],
   "source": [
    "print('Naive Bayes Accuracy', metrics.accuracy_score(y_mnb, y_test))\n",
    "print('confustion matric ', metrics.confusion_matrix(y_mnb, y_test))\n",
    "# print(metrics.classification_report(dataset['label'], dataset[]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# support vector machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM accuracy 0.9095477386934674\n",
      "confusion matric [[1148   64]\n",
      " [  62  119]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.95      0.95      0.95      1210\n",
      "        spam       0.66      0.65      0.65       183\n",
      "\n",
      "    accuracy                           0.91      1393\n",
      "   macro avg       0.80      0.80      0.80      1393\n",
      "weighted avg       0.91      0.91      0.91      1393\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.95      0.95      0.95      1212\n",
      "        spam       0.65      0.66      0.65       181\n",
      "\n",
      "    accuracy                           0.91      1393\n",
      "   macro avg       0.80      0.80      0.80      1393\n",
      "weighted avg       0.91      0.91      0.91      1393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc = svm.SVC(kernel='sigmoid', gamma=1.0)\n",
    "svc.fit(X_train_cv, y_train)\n",
    "\n",
    "y_svc_predict = svc.predict(X_test_cv)\n",
    "accuracy = metrics.accuracy_score(y_svc_predict, y_test)\n",
    "c_matrix = metrics.confusion_matrix(y_svc_predict, y_test, labels=['ham', 'spam'])\n",
    "print('SVM accuracy' , accuracy)\n",
    "print('confusion matric', c_matrix)\n",
    "print(metrics.classification_report(y_test, y_svc_predict, labels=['ham', 'spam']))\n",
    "print(metrics.classification_report(y_svc_predict, y_test, labels=['ham', 'spam']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K nearest neighbors classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\orsaater\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\base.py:908: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self._y = np.empty(y.shape, dtype=np.int)\n",
      "C:\\Users\\orsaater\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:56: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype = np.float\n",
      "C:\\Users\\orsaater\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:56: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype = np.float\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k Nearest accuracy  0.8686288585786073\n",
      "confusion matrix  [[1210  183]\n",
      " [   0    0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn import neighbors\n",
    "from sklearn import linear_model\n",
    "\n",
    "knc = neighbors.KNeighborsClassifier(n_neighbors=200)\n",
    "knc.fit(X_train_cv, y_train)\n",
    "\n",
    "y_knc_predict = knc.predict(X_test_cv)\n",
    "accuracy = metrics.accuracy_score(y_knc_predict, y_test)\n",
    "matrix = metrics.confusion_matrix(y_knc_predict, y_test)\n",
    "print('k Nearest accuracy ', accuracy)\n",
    "print('confusion matrix ', matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\orsaater\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:56: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype = np.float\n",
      "C:\\Users\\orsaater\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:56: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype = np.float\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic accuracy  0.8686288585786073\n",
      "Logistic confusion matrix  [[1210  183]\n",
      " [   0    0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn import neighbors\n",
    "from sklearn import linear_model\n",
    "\n",
    "lgc = linear_model.LogisticRegression()\n",
    "lgc.fit(X_train_cv, y_train)\n",
    "\n",
    "y_lgc_predict = knc.predict(X_test_cv)\n",
    "accuracy = metrics.accuracy_score(y_lgc_predict, y_test)\n",
    "matrix = metrics.confusion_matrix(y_lgc_predict, y_test)\n",
    "print('Logistic accuracy ', accuracy)\n",
    "print('Logistic confusion matrix ', matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
