{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTING NGN DATASET 1 USING PANDAS..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\binary/nltk_data'\n    - 'C:\\\\Users\\\\binary\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\binary\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\binary\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\binary\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\binary/nltk_data'\n    - 'C:\\\\Users\\\\binary\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\binary\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\binary\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\binary\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13268/199146365.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"english\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\binary/nltk_data'\n    - 'C:\\\\Users\\\\binary\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\binary\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\binary\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\binary\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "st = stopwords.words(\"english\")\n",
    "print(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>*IMPORTANT* NetBank Security requires you to a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Win a ÃÂ¥ÃÂ£1000 cash prize or a prize worth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>UR GOING 2 BAHAMAS! CallFREEFONE 08081560665 a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Sequel to the receipt of your Resume, the Boar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>WIN a ÃÂ¥ÃÂ£200 Shopping spree every WEEK St...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            message\n",
       "0      0  *IMPORTANT* NetBank Security requires you to a...\n",
       "1      0  Win a ÃÂ¥ÃÂ£1000 cash prize or a prize worth...\n",
       "2      0  UR GOING 2 BAHAMAS! CallFREEFONE 08081560665 a...\n",
       "3      0  Sequel to the receipt of your Resume, the Boar...\n",
       "4      0  WIN a ÃÂ¥ÃÂ£200 Shopping spree every WEEK St..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_ngn = pd.read_csv('sms_spam_ham_ng.csv')\n",
    "data_ngn.head()\n",
    "#data_ngn.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLEANING NGN DATASET.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\orsaater\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n",
      "C:\\Users\\orsaater\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\orsaater\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\orsaater\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "C:\\Users\\orsaater\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\orsaater\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\orsaater\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "C:\\Users\\orsaater\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\orsaater\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\orsaater\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[import, netbank, secur, requir, you, to, auth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[win, a, £1000, cash, prize, or, a, prize, wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[UR, go, 2, bahama, callfreefon, 08081560665, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>[sequel, to, the, receipt, of, your, resum, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>[win, a, £200, shop, spree, everi, week, start...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            message\n",
       "0      0  [import, netbank, secur, requir, you, to, auth...\n",
       "1      0  [win, a, £1000, cash, prize, or, a, prize, wor...\n",
       "2      0  [UR, go, 2, bahama, callfreefon, 08081560665, ...\n",
       "3      0  [sequel, to, the, receipt, of, your, resum, th...\n",
       "4      0  [win, a, £200, shop, spree, everi, week, start..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data_ngn.to_string()\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# print(string.punctuation)\n",
    "stop_word = stopwords.words('english')\n",
    "\n",
    "def preprocessing(text):\n",
    "    st = PorterStemmer()\n",
    "    clean_puct = ''.join([p for p in text if not p in string.punctuation])    \n",
    "    # tokinization of each row (message)\n",
    "    token = word_tokenize(clean_puct)\n",
    "    \n",
    "    # to remove somethin of this nature in dataset %&*^*^%£500\n",
    "    #   and stemming individuals word using porter stemmer \n",
    "    clean_tok = []\n",
    "    for tok in token:\n",
    "        found = re.search('([£][0-9]+)', tok)\n",
    "        if found:\n",
    "            clean_tok.append(found.group(0))\n",
    "        else:\n",
    "            clean_tok.append(st.stem(tok))\n",
    "    token = clean_tok\n",
    "\n",
    "\n",
    "    punc = [t for t in token if not t in string.punctuation]\n",
    "#     clean_words = [t for t in punc if not t in stop_word ]\n",
    "    return punc\n",
    "\n",
    "#     end of preprocessing method\n",
    "\n",
    "    \n",
    "data_ngn['message'] = data_ngn['message'].apply(lambda x:preprocessing(x))\n",
    "data_ngn.head()\n",
    "# data_ngn['clean_message'].to_string()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA EXPLORATION ON NGN DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty message =  0\n",
      "No of messages  1008\n",
      "Columns  2\n"
     ]
    }
   ],
   "source": [
    "# checking for empty row\n",
    "print('Empty message = ' , data_ngn['message'].isnull().sum())\n",
    "print('No of messages ' , len(data_ngn))\n",
    "print('Columns ', len(data_ngn.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# converting 0 = spam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>spam</td>\n",
       "      <td>[import, netbank, secur, requir, you, to, auth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>spam</td>\n",
       "      <td>[win, a, £1000, cash, prize, or, a, prize, wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>[UR, go, 2, bahama, callfreefon, 08081560665, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>spam</td>\n",
       "      <td>[sequel, to, the, receipt, of, your, resum, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>spam</td>\n",
       "      <td>[win, a, £200, shop, spree, everi, week, start...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0  spam  [import, netbank, secur, requir, you, to, auth...\n",
       "1  spam  [win, a, £1000, cash, prize, or, a, prize, wor...\n",
       "2  spam  [UR, go, 2, bahama, callfreefon, 08081560665, ...\n",
       "3  spam  [sequel, to, the, receipt, of, your, resum, th...\n",
       "4  spam  [win, a, £200, shop, spree, everi, week, start..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ngn['label'] = ['spam' for label in data_ngn['label'] ]\n",
    "data_ngn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPLITTING NGN DATASE TO TRAIN AND TEST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT DATASET SMS_spam Kaggle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "pd.set_option.display_max_row = 100\n",
    "dataset  = pd.read_csv('SMSSPamCollection', sep='\\t' , header=None)\n",
    "dataset.columns = ['label', 'message']\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA EXPLORATION ON KAGGLE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham lenght:4825 spam lenght:747\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as pyt\n",
    "import numpy as np\n",
    "\n",
    "# getting size of ham and spam in the message .... \n",
    "ham_size = len([ham for ham in dataset['label'] if ham =='ham'])\n",
    "spam_size = len([spam for spam in dataset['label'] if spam == 'spam'])\n",
    "print(f'ham lenght:{ham_size} spam lenght:{spam_size}')\n",
    "\n",
    "#dataset visualization\n",
    "X = np.array(['HAM', 'SPAM'])\n",
    "y = np.array([ham_size, spam_size])\n",
    "pyt.bar(X,y)\n",
    "pyt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goe\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>[Go, jurong, point, crazi, avail, bugi, n, gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>[Ok, lar, joke, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, FA, cup, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>[U, dun, say, earli, hor, U, c, alreadi, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>[nah, I, dont, think, goe, usf, live, around, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  [Go, jurong, point, crazi, avail, bugi, n, gre...\n",
       "1   ham                       [Ok, lar, joke, wif, u, oni]\n",
       "2  spam  [free, entri, 2, wkli, comp, win, FA, cup, fin...\n",
       "3   ham      [U, dun, say, earli, hor, U, c, alreadi, say]\n",
       "4   ham  [nah, I, dont, think, goe, usf, live, around, ..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "ps = PorterStemmer()\n",
    "print(ps.stem('goes'))\n",
    "stopword = stopwords.words('english')\n",
    "# print(st)\n",
    "\n",
    "def clean_dataset(data):\n",
    "    sentence = ''.join([t for t in data if not t in string.punctuation])\n",
    "    token = word_tokenize(sentence)\n",
    "    stem_token = [ps.stem(word) for word in token if word not in stopword]\n",
    "    return stem_token\n",
    "\n",
    "dataset['message'] = dataset['message'].apply(lambda a: clean_dataset(a))\n",
    "# dataset['msg_clean'] = [message for message in dataset['message']]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined NGN and KAGGLE dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>spam</td>\n",
       "      <td>[import, netbank, secur, requir, you, to, auth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>spam</td>\n",
       "      <td>[win, a, £1000, cash, prize, or, a, prize, wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>[UR, go, 2, bahama, callfreefon, 08081560665, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>spam</td>\n",
       "      <td>[sequel, to, the, receipt, of, your, resum, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>spam</td>\n",
       "      <td>[win, a, £200, shop, spree, everi, week, start...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0  spam  [import, netbank, secur, requir, you, to, auth...\n",
       "1  spam  [win, a, £1000, cash, prize, or, a, prize, wor...\n",
       "2  spam  [UR, go, 2, bahama, callfreefon, 08081560665, ...\n",
       "3  spam  [sequel, to, the, receipt, of, your, resum, th...\n",
       "4  spam  [win, a, £200, shop, spree, everi, week, start..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_data = data_ngn.append(dataset, ignore_index=True)\n",
    "master_data.head()\n",
    "# master.tail()\n",
    "# print(ngn_kaggle_dataset.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exploring the merge dataset kaggle and ngn data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham lenght:4825 spam lenght:1755\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as py\n",
    "import numpy as np \n",
    "\n",
    "ham_size = len([ham for ham in master_data['label'] if ham =='ham'])\n",
    "spam_size = len([spam for spam in master_data['label'] if spam == 'spam'])\n",
    "print(f'ham lenght:{ham_size} spam lenght:{spam_size}')\n",
    "\n",
    "X = np.array(['HAM' , 'SPAM'])\n",
    "y  = np.array([ham_size , spam_size])\n",
    "py.bar(X,y)\n",
    "py.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#    # splitting kaggle data set into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X lenght 5571\n",
      "x text len  1\n",
      "y train lenght 5571\n",
      "y test len 1\n",
      "data spliting completed\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(dataset['message'], dataset['label'], test_size=1)\n",
    "print('X lenght' , len(X_train))\n",
    "print('x text len ', len(X_test))\n",
    "print('y train lenght', len(y_train))\n",
    "print('y test len', len(y_test))\n",
    "print('data spliting completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5571, 5000)\n",
      "\n",
      "vectorizing complete.... \n"
     ]
    }
   ],
   "source": [
    "\n",
    "st = stopwords.words('english')\n",
    "def message_cleaner(word):\n",
    "    return [t for t in word if not t in st]\n",
    "\n",
    "cv = CountVectorizer(analyzer=message_cleaner ,max_features=5000)\n",
    "\n",
    "# learn vocabulary and train \n",
    "X = cv.fit(X_train)\n",
    "# print(X.vocabulary_)\n",
    "X_train_cv = cv.transform(X_train)\n",
    "print(X_train_cv.shape)\n",
    "# vectorize testing X\n",
    "X_test_cv = cv.transform(X_test)\n",
    "\n",
    "print()\n",
    "print('vectorizing complete.... ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# splitting master dataset into testing and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X lenght 4935\n",
      "x text len  1645\n",
      "y train lenght 4935\n",
      "y test len 1645\n",
      "2865    [say, slowli, godi, love, you, amp, I, need, y...\n",
      "2329        [just, sent, Do, scream, moan, bed, princess]\n",
      "1613                                    [meet, lunch, la]\n",
      "3555    [compani, goodenviron, terrif, food, realli, n...\n",
      "4207       [7, lor, chang, 2, suntec, wat, time, u, come]\n",
      "3638    [hey, glad, u, r, better, I, hear, u, treat, u...\n",
      "6419                 [I, ask, u, meet, da, ge, tmr, nite]\n",
      "2988                       [shhhhh, nobodi, suppos, know]\n",
      "4043                    [get, readi, ltgt, inch, pleasur]\n",
      "5893    [for, mani, thing, antibiot, use, chest, abdom...\n",
      "Name: message, dtype: object\n",
      "master data spliting completed................\n"
     ]
    }
   ],
   "source": [
    "X_master_train, X_master_test, y_master_train, y_master_test = model_selection.train_test_split(master_data['message'], master_data['label'])\n",
    "print('X lenght' , len(X_master_train))\n",
    "print('x text len ', len(X_master_test))\n",
    "print('y train lenght', len(y_master_train))\n",
    "print('y test len', len(y_master_test))\n",
    "print(X_master_train[:10])\n",
    "print('master data spliting completed................')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VECTORIZING MASTER DATASET USING COUNTVECTORIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4935, 5000)\n",
      "\n",
      "vectorizing complete.... \n"
     ]
    }
   ],
   "source": [
    "\n",
    "st = stopwords.words('english')\n",
    "def message_cleaner(word):\n",
    "    return [t for t in word if not t in st]\n",
    "\n",
    "cv = CountVectorizer(analyzer=message_cleaner ,max_features=5000)\n",
    "\n",
    "# learn vocabulary and train \n",
    "X = cv.fit(X_master_train)\n",
    "# print(X.vocabulary_)\n",
    "X_train_master_cv = cv.transform(X_master_train)\n",
    "print(X_train_master_cv.shape)\n",
    "# vectorize testing X\n",
    "X_test_master_cv = cv.transform(X_master_test)\n",
    "\n",
    "print()\n",
    "print('vectorizing complete.... ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predicting kaggle data using ML algorithm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn import model_selection \n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn import neighbors\n",
    "from sklearn import naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy 0.6462006079027356\n"
     ]
    }
   ],
   "source": [
    "mnb = naive_bayes.MultinomialNB(alpha=0.2)\n",
    "mnb.fit(X_train_cv , y_train)\n",
    "\n",
    "y_mnb = mnb.predict(X_test_master_cv)\n",
    "print('Naive Bayes Accuracy', metrics.accuracy_score(y_mnb, y_master_test))\n",
    "# print('confustion matric ', metrics.confusion_matrix(y_mnb, y_master_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Accuracy 0.7221884498480243\n"
     ]
    }
   ],
   "source": [
    "svc = svm.SVC(kernel='sigmoid', gamma=1.0)\n",
    "svc.fit(X_train_cv , y_train)\n",
    "\n",
    "y_svc = svc.predict(X_test_master_cv)\n",
    "print('Support Vector Accuracy', metrics.accuracy_score(y_svc, y_master_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy 0.7221884498480243\n"
     ]
    }
   ],
   "source": [
    "svc = svm.SVC(kernel='sigmoid', gamma=1.0)\n",
    "svc.fit(X_train_cv , y_train)\n",
    "\n",
    "y_svc = svc.predict(X_test_master_cv)\n",
    "print('Naive Bayes Accuracy', metrics.accuracy_score(y_svc, y_master_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\orsaater\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\base.py:908: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self._y = np.empty(y.shape, dtype=np.int)\n",
      "C:\\Users\\orsaater\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:56: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype = np.float\n",
      "C:\\Users\\orsaater\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:56: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype = np.float\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K nearest Neigbour Accuracy 0.7398176291793314\n"
     ]
    }
   ],
   "source": [
    "kn = neighbors.KNeighborsClassifier(n_neighbors=200)\n",
    "kn.fit(X_train_cv , y_train)\n",
    "\n",
    "y_kn = kn.predict(X_test_master_cv)\n",
    "print('K nearest Neigbour Accuracy', metrics.accuracy_score(y_kn, y_master_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic Regression Accuracy 0.7361702127659574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\orsaater\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\orsaater\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py:291: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  indices = (scores > 0).astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "lgc = linear_model.LogisticRegression()\n",
    "lgc.fit(X_train_cv , y_train)\n",
    "\n",
    "y_lr = lgc.predict(X_test_master_cv)\n",
    "print('logistic Regression Accuracy', metrics.accuracy_score(y_lr, y_master_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prediction master data using machine learning alogrithm (SVM, NB, LR etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn import model_selection \n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn import neighbors\n",
    "from sklearn import naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy 0.9665653495440729\n"
     ]
    }
   ],
   "source": [
    "mnb = naive_bayes.MultinomialNB(alpha=0.2)\n",
    "mnb.fit(X_train_master_cv , y_master_train)\n",
    "\n",
    "y_mnb = mnb.predict(X_test_master_cv)\n",
    "print('Naive Bayes Accuracy', metrics.accuracy_score(y_mnb, y_master_test))\n",
    "# print('confustion matric ', metrics.confusion_matrix(y_mnb, y_master_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy 0.8814589665653495\n"
     ]
    }
   ],
   "source": [
    "svc = svm.SVC(kernel='sigmoid', gamma=1.0)\n",
    "svc.fit(X_train_master_cv , y_master_train)\n",
    "\n",
    "y_svc = svc.predict(X_test_master_cv)\n",
    "print('Naive Bayes Accuracy', metrics.accuracy_score(y_svc, y_master_test))\n",
    "# print('confustion matric ', metrics.confusion_matrix(y_mnb, y_master_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\orsaater\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\base.py:908: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self._y = np.empty(y.shape, dtype=np.int)\n",
      "C:\\Users\\orsaater\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:56: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype = np.float\n",
      "C:\\Users\\orsaater\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:56: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype = np.float\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy 0.7209726443768997\n"
     ]
    }
   ],
   "source": [
    "kn = neighbors.KNeighborsClassifier(n_neighbors=200)\n",
    "kn.fit(X_train_master_cv , y_master_train)\n",
    "\n",
    "y_kn = kn.predict(X_test_master_cv)\n",
    "print('K nearest Neigbour Accuracy', metrics.accuracy_score(y_kn, y_master_test))\n",
    "# print('confustion matric ', metrics.confusion_matrix(y_mnb, y_master_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic Regression Accuracy 0.9738601823708206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\orsaater\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py:291: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  indices = (scores > 0).astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "lgc = linear_model.LogisticRegression()\n",
    "lgc.fit(X_train_master_cv , y_master_train)\n",
    "\n",
    "y_lr = lgc.predict(X_test_master_cv)\n",
    "print('logistic Regression Accuracy', metrics.accuracy_score(y_lr, y_master_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
