{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0503449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cff28c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2225, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>['tv', 'futur', 'hand', 'viewer', 'home', 'theatr', 'system', 'plasma', 'highdefinit', 'tv', 'digit', 'video', 'record', 'move', 'live', 'room', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>['worldcom', 'boss', 'left', 'book', 'alon', 'former', 'worldcom', 'boss', 'berni', 'ebber', 'accus', 'overse', '11bn', '£58bn', 'fraud', 'never',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>['tiger', 'wari', 'farrel', 'gambl', 'leicest', 'say', 'rush', 'make', 'bid', 'andi', 'farrel', 'great', 'britain', 'rugbi', 'leagu', 'captain', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>['yead', 'face', 'newcastl', 'fa', 'cup', 'premiership', 'side', 'newcastl', 'unit', 'face', 'trip', 'ryman', 'premier', 'leagu', 'leader', 'yead'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>['ocean', 'twelv', 'raid', 'box', 'offic', 'ocean', 'twelv', 'crime', 'caper', 'sequel', 'star', 'georg', 'clooney', 'brad', 'pitt', 'julia', 'rob...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category  \\\n",
       "0           tech   \n",
       "1       business   \n",
       "2          sport   \n",
       "3          sport   \n",
       "4  entertainment   \n",
       "\n",
       "                                                                                                                                              clean_text  \n",
       "0  ['tv', 'futur', 'hand', 'viewer', 'home', 'theatr', 'system', 'plasma', 'highdefinit', 'tv', 'digit', 'video', 'record', 'move', 'live', 'room', '...  \n",
       "1  ['worldcom', 'boss', 'left', 'book', 'alon', 'former', 'worldcom', 'boss', 'berni', 'ebber', 'accus', 'overse', '11bn', '£58bn', 'fraud', 'never',...  \n",
       "2  ['tiger', 'wari', 'farrel', 'gambl', 'leicest', 'say', 'rush', 'make', 'bid', 'andi', 'farrel', 'great', 'britain', 'rugbi', 'leagu', 'captain', '...  \n",
       "3  ['yead', 'face', 'newcastl', 'fa', 'cup', 'premiership', 'side', 'newcastl', 'unit', 'face', 'trip', 'ryman', 'premier', 'leagu', 'leader', 'yead'...  \n",
       "4  ['ocean', 'twelv', 'raid', 'box', 'offic', 'ocean', 'twelv', 'crime', 'caper', 'sequel', 'star', 'georg', 'clooney', 'brad', 'pitt', 'julia', 'rob...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 150)\n",
    "dataset = pd.read_csv(\"clean_bbc_news_dataset\", index_col=0)\n",
    "print(dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5783086f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1780 Training Sample  AND  445 testing Sample\n",
      "['tech' 'business' 'sport' 'entertainment' 'politics']\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection \n",
    "\n",
    "X = dataset['clean_text']\n",
    "y = dataset['category']\n",
    "\n",
    "train_X, test_X, train_y, test_y = model_selection.train_test_split(X, y, test_size=.20)\n",
    "print(f'{len(train_X)} Training Sample  AND  {len(test_y)} testing Sample')\n",
    "print(dataset['category'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa34715",
   "metadata": {},
   "source": [
    "# demo document first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d97554e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sentence = [\n",
    "    [\"i\" , \"love\", \"my\", \"dog\"],\n",
    "    [\"i\", \"love\", \"my\", \"cat\"], \n",
    "    [\"you\", \"love\", \"my\", \"dog\"],\n",
    "    [\"do\", \"you\", \"think\", \"my\", \"dog\", \"is\", \"amazing\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b43fe907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n"
     ]
    }
   ],
   "source": [
    "token = Tokenizer(num_words=11 , oov_token='<OOV>')\n",
    "token.fit_on_texts(sentence)\n",
    "print(token.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abe69d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "sequence = token.texts_to_sequences(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b25e2507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.58778666 0.69314718 0.69314718 0.84729786\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.58778666 0.69314718 0.         0.84729786\n",
      "  0.         1.09861229 0.         0.         0.        ]\n",
      " [0.         0.         0.58778666 0.69314718 0.69314718 0.\n",
      "  0.84729786 0.         0.         0.         0.        ]\n",
      " [0.         1.60943791 0.58778666 0.         0.69314718 0.\n",
      "  0.84729786 0.         1.09861229 1.09861229 1.09861229]]\n"
     ]
    }
   ],
   "source": [
    "sequence_tfidf = token.texts_to_matrix(sentence, mode='tfidf')\n",
    "print(sequence_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4a25f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16800bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "# tfidf_calculator = TextVectorization(\n",
    "#   standardize = 'lower_and_strip_punctuation',\n",
    "#   split       = 'whitespace',\n",
    "#   max_tokens  = 100,\n",
    "#   output_mode ='tf-idf',\n",
    "#   pad_to_max_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80d0bcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_calculator.adapt(sentence)\n",
    "# tfidf_calculator.get_vocabulary()\n",
    "# tfids = tfidf_calculator(sentence)\n",
    "# print(tfids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbacf614",
   "metadata": {},
   "source": [
    "# using main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b7e80a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX_VAL  20861\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2056.2260674157305"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word lenth for each coloum\n",
    "import numpy as np\n",
    "w_count = []\n",
    "def get_word_lenght(text):\n",
    "    w_count.append(len(text))\n",
    "    return len(text)\n",
    "    \n",
    "dataset['word_count'] = dataset['clean_text'].apply(lambda x:get_word_lenght(x))\n",
    "max_val = dataset['word_count'].max()\n",
    "print('MAX_VAL ', max_val)\n",
    "w = np.array(w_count).mean()\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212ceb5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fffd9e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab29dde4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 3, ..., 1, 2, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encode = LabelEncoder()\n",
    "sentence = dataset['clean_text']\n",
    "label = label_encode.fit_transform(dataset['category'])\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30af09b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = model_selection.train_test_split(sentence, label, test_size=.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfc7d7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train 1780\n",
      "Y train 1780\n",
      "X test 445\n",
      "Y test 445\n"
     ]
    }
   ],
   "source": [
    "print('X train' , len(xtrain))\n",
    "print('Y train' , len(ytrain))\n",
    "print('X test' , len(xtest))\n",
    "print('Y test' , len(ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf627e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size =================>>>>>>>>>>>>   22101\n"
     ]
    }
   ],
   "source": [
    "# tokenizaing text ...\n",
    "tokenizer = Tokenizer(num_words=25000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(xtrain)\n",
    "w_index = tokenizer.word_index\n",
    "print('vocab_size =================>>>>>>>>>>>>  ' , len(w_index))\n",
    "# print(w_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "464a1036",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_seq = tokenizer.texts_to_sequences(xtrain)\n",
    "xtest_seq = tokenizer.texts_to_sequences(xtest)\n",
    "# print(xtest_seq[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d2e78af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train pad shape (1780, 5000)\n",
      "x test pad shape (445, 5000)\n"
     ]
    }
   ],
   "source": [
    "max_val = 5000\n",
    "xtrain_pad = pad_sequences(xtrain_seq, maxlen=max_val)\n",
    "xtest_pad = pad_sequences(xtest_seq, maxlen=max_val)\n",
    "print('x train pad shape', xtrain_pad.shape)\n",
    "print('x test pad shape', xtest_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6c78027d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 5000, 100)         2500000   \n",
      "                                                                 \n",
      " global_average_pooling1d_1   (None, 100)              0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 500)               50500     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 2505      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,553,005\n",
      "Trainable params: 2,553,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 25000\n",
    "embedding_dim = 100\n",
    "pad_length =5000\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    keras.layers.Embedding(vocab_size, embedding_dim, input_length=pad_length),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dense(500, activation='relu'),\n",
    "    keras.layers.Dense(5, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'] )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ac22b5be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "56/56 [==============================] - 10s 124ms/step - loss: 1.6035 - accuracy: 0.2348 - val_loss: 1.6050 - val_accuracy: 0.2045\n",
      "Epoch 2/10\n",
      "56/56 [==============================] - 7s 118ms/step - loss: 1.6003 - accuracy: 0.2360 - val_loss: 1.6016 - val_accuracy: 0.2045\n",
      "Epoch 3/10\n",
      "56/56 [==============================] - 7s 116ms/step - loss: 1.5948 - accuracy: 0.2326 - val_loss: 1.5967 - val_accuracy: 0.2045\n",
      "Epoch 4/10\n",
      "56/56 [==============================] - 6s 116ms/step - loss: 1.5735 - accuracy: 0.2522 - val_loss: 1.5754 - val_accuracy: 0.2067\n",
      "Epoch 5/10\n",
      "56/56 [==============================] - 7s 117ms/step - loss: 1.4773 - accuracy: 0.4185 - val_loss: 1.3875 - val_accuracy: 0.5236\n",
      "Epoch 6/10\n",
      "56/56 [==============================] - 7s 117ms/step - loss: 1.1956 - accuracy: 0.5978 - val_loss: 1.0547 - val_accuracy: 0.6270\n",
      "Epoch 7/10\n",
      "56/56 [==============================] - 7s 117ms/step - loss: 0.8603 - accuracy: 0.8022 - val_loss: 0.7600 - val_accuracy: 0.8966\n",
      "Epoch 8/10\n",
      "56/56 [==============================] - 7s 117ms/step - loss: 0.5780 - accuracy: 0.9056 - val_loss: 0.5049 - val_accuracy: 0.9191\n",
      "Epoch 9/10\n",
      "56/56 [==============================] - 7s 120ms/step - loss: 0.3738 - accuracy: 0.9483 - val_loss: 0.3508 - val_accuracy: 0.9461\n",
      "Epoch 10/10\n",
      "56/56 [==============================] - 7s 122ms/step - loss: 0.2554 - accuracy: 0.9646 - val_loss: 0.3019 - val_accuracy: 0.9438\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x188efebfd00>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_pad, ytrain , epochs=10, validation_data=(xtest_pad, ytest), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6490c70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 36ms/step - loss: 0.3019 - accuracy: 0.9438\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3019077479839325, 0.9438202381134033]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(xtest_pad, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "121c0eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train pad shape (1780, 5000)\n",
      "x test pad shape (445, 5000)\n"
     ]
    }
   ],
   "source": [
    "max_val = 5000\n",
    "xtrain_pad = pad_sequences(xtrain_seq, maxlen=max_val)\n",
    "xtest_pad = pad_sequences(xtest_seq, maxlen=max_val)\n",
    "print('x train pad shape', xtrain_pad.shape)\n",
    "print('x test pad shape', xtest_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9e8cbe7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m embedding_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m      3\u001b[0m pad_length \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m\n\u001b[1;32m----> 5\u001b[0m lstm_model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[0;32m      6\u001b[0m     \n\u001b[0;32m      7\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mEmbedding(vocab_size, embedding_dim, input_length\u001b[38;5;241m=\u001b[39mpad_lenght),\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, dropout=0.2, recurrent_dropout=0.5)),\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mLSTM(\u001b[38;5;241m50\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m),\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m     \n\u001b[0;32m     12\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m50\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     13\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m5\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m     \n\u001b[0;32m     15\u001b[0m ])\n\u001b[0;32m     18\u001b[0m lstm_model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m] )\n\u001b[0;32m     19\u001b[0m lstm_model\u001b[38;5;241m.\u001b[39msummary()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "vocab_size = 25000\n",
    "embedding_dim = 50\n",
    "pad_length =5000\n",
    "\n",
    "lstm_model = tf.keras.Sequential([\n",
    "    \n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=pad_lenght),\n",
    "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, dropout=0.2, recurrent_dropout=0.5)),\n",
    "    tf.keras.layers.LSTM(50, dropout=0.2),\n",
    "\n",
    "    \n",
    "    tf.keras.layers.Dense(50, activation='relu'),\n",
    "    tf.keras.layers.Dense(5, activation='sigmoid')\n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "lstm_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'] )\n",
    "lstm_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41f66f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "56/56 [==============================] - 393s 7s/step - loss: 1.5172 - accuracy: 0.2893 - val_loss: 1.3081 - val_accuracy: 0.4180\n",
      "Epoch 2/7\n",
      "56/56 [==============================] - 400s 7s/step - loss: 1.0418 - accuracy: 0.5129 - val_loss: 0.9496 - val_accuracy: 0.5640\n",
      "Epoch 3/7\n",
      "56/56 [==============================] - 425s 8s/step - loss: 0.6771 - accuracy: 0.6573 - val_loss: 0.8192 - val_accuracy: 0.5596\n",
      "Epoch 4/7\n",
      "56/56 [==============================] - 389s 7s/step - loss: 0.5488 - accuracy: 0.7697 - val_loss: 0.6805 - val_accuracy: 0.6382\n",
      "Epoch 5/7\n",
      "56/56 [==============================] - 372s 7s/step - loss: 0.3472 - accuracy: 0.8843 - val_loss: 0.4932 - val_accuracy: 0.8629\n",
      "Epoch 6/7\n",
      "56/56 [==============================] - 337s 6s/step - loss: 0.2613 - accuracy: 0.9270 - val_loss: 0.6224 - val_accuracy: 0.7933\n",
      "Epoch 7/7\n",
      "56/56 [==============================] - 394s 7s/step - loss: 0.1164 - accuracy: 0.9792 - val_loss: 0.4064 - val_accuracy: 0.8742\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bf9ed475b0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.fit(xtrain_pad, ytrain , epochs=7, validation_data=(xtest_pad, ytest), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "092d7dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 7s 520ms/step - loss: 0.4064 - accuracy: 0.8742\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.40643972158432007, 0.8741573095321655]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.evaluate(xtest_pad, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7f78e76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size 21845\n"
     ]
    }
   ],
   "source": [
    "tokenizer2 = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
    "tokenizer2.fit_on_texts(xtrain)\n",
    "w_index2 = tokenizer2.word_index\n",
    "print('vocab_size' , len(w_index2))\n",
    "# print(w_index2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "493bd2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1780, 5000)\n"
     ]
    }
   ],
   "source": [
    "xtrain_seq2 = tokenizer2.texts_to_matrix(xtrain, mode='tfidf')\n",
    "xtest_seq2 = tokenizer2.texts_to_matrix(xtest, mode='tfidf')\n",
    "\n",
    "print(xtrain_seq2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "71d0aeae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train pad shape (1780, 200)\n",
      "x test pad shape (445, 200)\n"
     ]
    }
   ],
   "source": [
    "max_val = 200\n",
    "xtrain_pad2 = pad_sequences(xtrain_seq2, maxlen=max_val)\n",
    "xtest_pad2 = pad_sequences(xtest_seq2, maxlen=max_val)\n",
    "print('x train pad shape', xtrain_pad2.shape)\n",
    "print('x test pad shape', xtest_pad2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ccb8e3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_8 (Embedding)     (None, None, 128)         640000    \n",
      "                                                                 \n",
      " bidirectional_8 (Bidirectio  (None, 256)              263168    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 5)                 1285      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 904,453\n",
      "Trainable params: 904,453\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5000\n",
    "w_dim = 128\n",
    "pad_size = max_val\n",
    "\n",
    "tf_idf_model = keras.Sequential([\n",
    "    keras.layers.Embedding(vocab_size , w_dim),\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(128)),\n",
    "    \n",
    "    keras.layers.Dense(5, activation='sigmoid')\n",
    "])\n",
    "\n",
    "tf_idf_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "tf_idf_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f82c18c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_idf_model.fit(xtrain_pad2, ytrain , epochs=28, validation_data=(xtest_pad2, ytest), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "63f6da5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 63ms/step - loss: 1.5998 - accuracy: 0.2090\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.599849820137024, 0.20898877084255219]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_model.evaluate(xtest_pad2, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0adec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fb96df89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9e63a040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c80bff37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<OOV>': 1,\n",
       " 'my': 2,\n",
       " 'love': 3,\n",
       " 'dog': 4,\n",
       " 'i': 5,\n",
       " 'you': 6,\n",
       " 'cat': 7,\n",
       " 'do': 8,\n",
       " 'think': 9,\n",
       " 'is': 10,\n",
       " 'amazing': 11}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7a8953d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "306542c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  5,  3,  2,  4],\n",
       "       [ 0,  0,  0,  5,  3,  2,  7],\n",
       "       [ 0,  0,  0,  6,  3,  2,  4],\n",
       "       [ 8,  6,  9,  2,  4, 10, 11]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7239505a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embed (Embedding)           (None, 7, 4)              400       \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 28)                0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 1)                 29        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f9fd5d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6920 - accuracy: 0.5000\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6906 - accuracy: 0.5000\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6893 - accuracy: 0.5000\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6879 - accuracy: 0.5000\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6865 - accuracy: 0.2500\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6851 - accuracy: 0.7500\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6838 - accuracy: 0.7500\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6824 - accuracy: 0.7500\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6811 - accuracy: 0.7500\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6797 - accuracy: 0.7500\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6784 - accuracy: 0.7500\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6771 - accuracy: 0.7500\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6757 - accuracy: 0.7500\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6744 - accuracy: 0.7500\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6731 - accuracy: 0.7500\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6717 - accuracy: 0.7500\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6704 - accuracy: 0.7500\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6691 - accuracy: 0.7500\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6678 - accuracy: 0.7500\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6664 - accuracy: 0.7500\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6651 - accuracy: 0.7500\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6638 - accuracy: 0.7500\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6624 - accuracy: 0.7500\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6611 - accuracy: 0.7500\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6598 - accuracy: 0.7500\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6584 - accuracy: 0.7500\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6571 - accuracy: 0.7500\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6557 - accuracy: 0.7500\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6543 - accuracy: 0.7500\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6530 - accuracy: 0.7500\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6516 - accuracy: 0.7500\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6502 - accuracy: 0.7500\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6488 - accuracy: 0.7500\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6475 - accuracy: 0.7500\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6461 - accuracy: 0.7500\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6447 - accuracy: 0.7500\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6432 - accuracy: 0.7500\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6418 - accuracy: 0.7500\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6404 - accuracy: 0.7500\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6390 - accuracy: 0.7500\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6375 - accuracy: 0.7500\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6361 - accuracy: 0.7500\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6346 - accuracy: 0.7500\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6331 - accuracy: 0.7500\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6317 - accuracy: 0.7500\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6302 - accuracy: 0.7500\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6287 - accuracy: 0.7500\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6272 - accuracy: 0.7500\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6257 - accuracy: 0.7500\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6242 - accuracy: 0.7500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bf96e82df0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pad_token\n",
    "y = np.array([0,0,1,1])\n",
    "mod.fit(x,y, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "868d663f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6227 - accuracy: 0.7500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6226552724838257, 0.75]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "708007ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cbace466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02007433, -0.05071082,  0.00803106, -0.08737522], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b70e5942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03500351, -0.10022003, -0.00099805, -0.0312709 ], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90088d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
