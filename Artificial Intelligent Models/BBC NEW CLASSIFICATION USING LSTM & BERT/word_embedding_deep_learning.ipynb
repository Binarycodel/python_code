{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "bc1e6a6a-c72e-4b82-8a1d-405f0232604f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2225, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home theatre systems  plasma high-definition tvs  and digital video recorders moving into the living room  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldcom boss bernie ebbers  who is accused of overseeing an $11bn (£5.8bn) fraud  never made accounting d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say they will not be rushed into making a bid for andy farrell should the great britain rugby league cap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership side newcastle united face a trip to ryman premier league leaders yeading in the fa cup third round. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve  the crime caper sequel starring george clooney  brad pitt and julia roberts  has gone straight to ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category  \\\n",
       "0           tech   \n",
       "1       business   \n",
       "2          sport   \n",
       "3          sport   \n",
       "4  entertainment   \n",
       "\n",
       "                                                                                                                                                    text  \n",
       "0  tv future in the hands of viewers with home theatre systems  plasma high-definition tvs  and digital video recorders moving into the living room  ...  \n",
       "1  worldcom boss  left books alone  former worldcom boss bernie ebbers  who is accused of overseeing an $11bn (£5.8bn) fraud  never made accounting d...  \n",
       "2  tigers wary of farrell  gamble  leicester say they will not be rushed into making a bid for andy farrell should the great britain rugby league cap...  \n",
       "3  yeading face newcastle in fa cup premiership side newcastle united face a trip to ryman premier league leaders yeading in the fa cup third round. ...  \n",
       "4  ocean s twelve raids box office ocean s twelve  the crime caper sequel starring george clooney  brad pitt and julia roberts  has gone straight to ...  "
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', 150)\n",
    "dataset = pd.read_csv(\"bbc_text.csv\")\n",
    "print(dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "f62360e0-aceb-4034-83c8-424d6e316a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "stop_word = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "ed9455fd-b129-4c44-acbd-8602e118c70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "def dataset_cleaning(text):\n",
    "    #  removeing punctuation from the dataset\n",
    "    text = ''.join([token for token in text if not token in string.punctuation])\n",
    "    # tokenizing the dataset e.g ['tv' , 'future' , 'in' , 'the']\n",
    "    token = word_tokenize(text)\n",
    "    # removing stopword (words that had less meaning to the dataset)     \n",
    "    text  = [ps.stem(t) for t in token if not t in stop_word ]\n",
    "    return text \n",
    "\n",
    "dataset['clean_text'] = dataset[\"text\"].apply(lambda x: dataset_cleaning(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "fe37a6a6-dfc3-4e42-b507-b59c2f4e1454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home theatre systems  plasma high-definition tvs  and digital video recorders moving into the living room  ...</td>\n",
       "      <td>[tv, futur, hand, viewer, home, theatr, system, plasma, highdefinit, tv, digit, video, record, move, live, room, way, peopl, watch, tv, radic, dif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldcom boss bernie ebbers  who is accused of overseeing an $11bn (£5.8bn) fraud  never made accounting d...</td>\n",
       "      <td>[worldcom, boss, left, book, alon, former, worldcom, boss, berni, ebber, accus, overse, 11bn, £58bn, fraud, never, made, account, decis, wit, told...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say they will not be rushed into making a bid for andy farrell should the great britain rugby league cap...</td>\n",
       "      <td>[tiger, wari, farrel, gambl, leicest, say, rush, make, bid, andi, farrel, great, britain, rugbi, leagu, captain, decid, switch, code, anybodi, els...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category  \\\n",
       "0      tech   \n",
       "1  business   \n",
       "2     sport   \n",
       "\n",
       "                                                                                                                                                    text  \\\n",
       "0  tv future in the hands of viewers with home theatre systems  plasma high-definition tvs  and digital video recorders moving into the living room  ...   \n",
       "1  worldcom boss  left books alone  former worldcom boss bernie ebbers  who is accused of overseeing an $11bn (£5.8bn) fraud  never made accounting d...   \n",
       "2  tigers wary of farrell  gamble  leicester say they will not be rushed into making a bid for andy farrell should the great britain rugby league cap...   \n",
       "\n",
       "                                                                                                                                              clean_text  \n",
       "0  [tv, futur, hand, viewer, home, theatr, system, plasma, highdefinit, tv, digit, video, record, move, live, room, way, peopl, watch, tv, radic, dif...  \n",
       "1  [worldcom, boss, left, book, alon, former, worldcom, boss, berni, ebber, accus, overse, 11bn, £58bn, fraud, never, made, account, decis, wit, told...  \n",
       "2  [tiger, wari, farrel, gambl, leicest, say, rush, make, bid, andi, farrel, great, britain, rugbi, leagu, captain, decid, switch, code, anybodi, els...  "
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "2513dcf1-5bab-4ebb-8921-4c8b4a513c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['category', 'text', 'clean_text'], dtype='object')"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DATA PLITING.... \n",
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "5300e8df-4a48-48d5-a769-32ee80b86f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import model_selection\n",
    "\n",
    "lb = LabelEncoder()\n",
    "label = lb.fit_transform(dataset['category'])\n",
    "bbc_news = dataset['clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "b14d8723-5e18-48d5-8d4b-c45176da10d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = model_selection.train_test_split(bbc_news, label, test_size=.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47eb107-b0e9-4191-8439-58e24fae9cac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "8ee2cc18-f7f4-4c21-833b-16a8a1168a27",
   "metadata": {},
   "source": [
    "LOADING DATASET FOR WORD EMBEDDING..... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "823e05d7-86c0-48e4-8e95-1d5b3fdf0bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "# defining helper methods.... \n",
    "\n",
    "path = 'News_Articles/business/001.txt'\n",
    "\n",
    "def read_file_content(path): \n",
    "    with open(path, 'r') as file:\n",
    "        # file.readline()\n",
    "        data = file.read().strip()\n",
    "        # print(data)\n",
    "        return data\n",
    "\n",
    "        \n",
    "# for reading each file content\n",
    "# read_file_content(path)\n",
    "\n",
    "\n",
    "# heler method for getting each file address \n",
    "def get_file_paths(main_dir):\n",
    "    file_path = []\n",
    "#     looping throug all file in a director...\n",
    "    for file in os.listdir(main_dir):\n",
    "        if file.endswith('.txt'):\n",
    "            full_path = f'{main_dir}/{file}'\n",
    "            file_path.append(full_path)\n",
    "            # print(file)\n",
    "    \n",
    "    return file_path \n",
    "\n",
    "# get_file_paths(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "13d50997-7ae2-479e-9426-55179c5fe7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_informations = {\n",
    "        'business':'News_Articles/business', \n",
    "        'entertainment':'News_Articles/entertainment',\n",
    "        'politics':'News_Articles/politics',\n",
    "        'sport': 'News_Articles/sport',\n",
    "        'tech': 'News_Articles/tech'\n",
    "}\n",
    "\n",
    "news_dataset = {\n",
    "    'business': [], \n",
    "    'entertainment': [], \n",
    "    'politics': [], \n",
    "    'sport':[],\n",
    "    'tech':[]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "694207cc-e60f-4404-ba60-3059c9c54d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510\n",
      "386\n",
      "417\n",
      "511\n",
      "401\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "for tag, path in news_informations.items() : \n",
    "    \n",
    "#     getting files\n",
    "    file_path = get_file_paths(path)\n",
    "    print(len(file_path))\n",
    "    \n",
    "#    looping through each file path and geeting content \n",
    "    for file in file_path: \n",
    "        content = read_file_content(file)\n",
    "        # print(len(content))\n",
    "        news_dataset[tag].append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "04df176b-cf01-41c6-9582-f45b6ebb6ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# news_dataset['tech']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "8419a21c-65bb-41f9-a09c-c1f634384183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution complete\n"
     ]
    }
   ],
   "source": [
    "# creating a bbc_new corpus \n",
    "import re\n",
    "bbc_news_corpus = []\n",
    "for tag, mini_copus in news_dataset.items():\n",
    "    \n",
    "    [bbc_news_corpus.append(copus) for copus in mini_copus]\n",
    "print('execution complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4e008800-7588-4aeb-bd82-8da53f7b38a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'cool', 'boy', 'i', 'love', 'ade', 'the', 'this', 'is', 'cool', 'by', 'me']\n",
      "['second', 'line', 'texy', 'i', 'love', 'ade', 'the', 'tend', 'fo', 'second', 'linee']\n",
      "['start', 'tird', 'cool', 'boy', 'i', 'love', 'ade', 'the', 'this', 'is', 'c', 'the', 'thied', 'line']\n"
     ]
    }
   ],
   "source": [
    "#  testing helper methods\n",
    "\n",
    "import re\n",
    "import string\n",
    "# print(string.punctuation)\n",
    "# print(string.punctuation)\n",
    "corpus = ['this is cool boy \\ni love ade the\\n\\n this is cool by me',\n",
    "         'second line texy \\ni love ade the\\n\\n tend. fo second linee',\n",
    "          'start tird cool boy \\ni love ade the\\n\\n this is c? the thied line'\n",
    "         ]\n",
    "\n",
    "def prepro(corpus):\n",
    "    # word = [re.sub('\\n', '', w.strip()) for w in corpus]\n",
    "    clean = ''.join([w for w in corpus if w not in string.punctuation])\n",
    "    clean = word_tokenize(clean)\n",
    "    # print(word)\\\n",
    "    return clean\n",
    "\n",
    "for c in corpus:\n",
    "    print(prepro(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "62da06e8-7968-4e99-88ad-4ea5c9190eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ad sales boost Time Warner profit\\n\\nQuarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (Â£600m) for the three months to December, from $639m year-earlier.\\n\\nThe firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales. TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn. Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.\\n\\nTime Warner said on Friday that it now owns 8% of search-engine Google. But its own internet business, AOL, had has mixed fortunes. It lost 464,000 subscribers in the fourth quarter profits were lower than in the preceding three quarters. However, the company said AOL\\'s underlying profit before exceptional items rose 8% on the back of stronger internet advertising revenues. It hopes to increase subscribers by offering the online service free to TimeWarner internet customers and will try to sign up AOL\\'s existing customers for high-speed broadband. TimeWarner also has to restate 2000 and 2003 results following a probe by the US Securities Exchange Commission (SEC), which is close to concluding.\\n\\nTime Warner\\'s fourth quarter profits were slightly better than analysts\\' expectations. But its film division saw profits slump 27% to $284m, helped by box-office flops Alexander and Catwoman, a sharp contrast to year-earlier, when the third and final film in the Lord of the Rings trilogy boosted results. For the full-year, TimeWarner posted a profit of $3.36bn, up 27% from its 2003 performance, while revenues grew 6.4% to $42.09bn. \"Our financial performance was strong, meeting or exceeding all of our full-year objectives and greatly enhancing our flexibility,\" chairman and chief executive Richard Parsons said. For 2005, TimeWarner is projecting operating earnings growth of around 5%, and also expects higher revenue and wider profit margins.\\n\\nTimeWarner is to restate its accounts as part of efforts to resolve an inquiry into AOL by US market regulators. It has already offered to pay $300m to settle charges, in a deal that is under review by the SEC. The company said it was unable to estimate the amount it needed to set aside for legal reserves, which it previously set at $500m. It intends to adjust the way it accounts for a deal with German music publisher Bertelsmann\\'s purchase of a stake in AOL Europe, which it had reported as advertising revenue. It will now book the sale of its stake in AOL Europe as a loss on the value of that stake.',\n",
       " 'Dollar gains on Greenspan speech\\n\\nThe dollar has hit its highest level against the euro in almost three months after the Federal Reserve head said the US trade deficit is set to stabilise.\\n\\nAnd Alan Greenspan highlighted the US government\\'s willingness to curb spending and rising household savings as factors which may help to reduce it. In late trading in New York, the dollar reached $1.2871 against the euro, from $1.2974 on Thursday. Market concerns about the deficit has hit the greenback in recent months. On Friday, Federal Reserve chairman Mr Greenspan\\'s speech in London ahead of the meeting of G7 finance ministers sent the dollar higher after it had earlier tumbled on the back of worse-than-expected US jobs data. \"I think the chairman\\'s taking a much more sanguine view on the current account deficit than he\\'s taken for some time,\" said Robert Sinche, head of currency strategy at Bank of America in New York. \"He\\'s taking a longer-term view, laying out a set of conditions under which the current account deficit can improve this year and next.\"\\n\\nWorries about the deficit concerns about China do, however, remain. China\\'s currency remains pegged to the dollar and the US currency\\'s sharp falls in recent months have therefore made Chinese export prices highly competitive. But calls for a shift in Beijing\\'s policy have fallen on deaf ears, despite recent comments in a major Chinese newspaper that the \"time is ripe\" for a loosening of the peg. The G7 meeting is thought unlikely to produce any meaningful movement in Chinese policy. In the meantime, the US Federal Reserve\\'s decision on 2 February to boost interest rates by a quarter of a point - the sixth such move in as many months - has opened up a differential with European rates. The half-point window, some believe, could be enough to keep US assets looking more attractive, and could help prop up the dollar. The recent falls have partly been the result of big budget deficits, as well as the US\\'s yawning current account gap, both of which need to be funded by the buying of US bonds and assets by foreign firms and governments. The White House will announce its budget on Monday, and many commentators believe the deficit will remain at close to half a trillion dollars.']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc_news_corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "44987532-1218-44f9-89fd-25b2cec7f921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete\n"
     ]
    }
   ],
   "source": [
    "# preprocessing the bbc_news corpus by removing new line caracter and punctuation\n",
    "\n",
    "def clean_corpus(corpus):\n",
    "     # word = [re.sub('\\n', '', w.strip()) for w in corpus]\n",
    "    clean = ''.join([w for w in corpus if w not in string.punctuation])\n",
    "    clean = word_tokenize(clean)\n",
    "    # print(word)\\\n",
    "    # print(clean)\n",
    "    return clean\n",
    "\n",
    "\n",
    "clean_bbc_news = []\n",
    "for data_sample in bbc_news_corpus : \n",
    "    # clean_corpus(data_data)\n",
    "    clean_bbc_news.append(clean_corpus(data_sample))\n",
    "\n",
    "# bbc_news_corpus.apply(lambda x : clean_corpus(x))\n",
    "\n",
    "print('Preprocessing complete')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "93b10ecd-75ed-4557-901f-6ada43fd55bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_bbc_news[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "af200217-d48c-4e67-a5a9-c5f91772b333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATING WORD EMBEDDINGS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "21ee2da8-87ce-4dc1-85e7-cd626175b3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "7bab1eec-bf58-4721-a59d-d746ea8d9b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_vec_model = gensim.models.Word2Vec(\n",
    "    window=20,\n",
    "    workers=5,\n",
    "    min_count=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "a353f9f1-c29e-4d6c-b229-aba456aa37e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model builing........\n",
    "\n",
    "bbc_vec_model.build_vocab(clean_bbc_news, progress_per=1000)\n",
    "# bbc_vec_model.wv.key_to_index\n",
    "# wtv_model.wv.index_to_key\n",
    "# bbc_vec_model.wv.key_to_index.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "68f869ae-0a65-4a94-9549-688bfa2b3c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3342158, 4256025)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vec model training \n",
    "bbc_vec_model.train(clean_bbc_news, total_examples=bbc_vec_model.corpus_count, epochs=bbc_vec_model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "c1bfe362-d26b-434a-8aaa-0029d6d7e562",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_vec_model.save('bbc_new_word2vec_model_summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "49dbfab0-8421-4f01-b2af-a95848c96b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train pad shape (1780, 5000)\n",
      "x test pad shape (445, 5000)\n"
     ]
    }
   ],
   "source": [
    "# DEEP LEARNING NEURAL NETWORK......\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences  \n",
    "\n",
    "\n",
    "# tokenizaing text ...\n",
    "vocab_size = 25000\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(xtrain)\n",
    "\n",
    "# generating word sequence\n",
    "xtrain_seq = tokenizer.texts_to_sequences(xtrain)\n",
    "xtest_seq = tokenizer.texts_to_sequences(xtest)\n",
    "\n",
    "# sequence padding \n",
    "max_val = 5000\n",
    "xtrain_pad = pad_sequences(xtrain_seq, maxlen=max_val)\n",
    "xtest_pad = pad_sequences(xtest_seq, maxlen=max_val)\n",
    "print('x train pad shape', xtrain_pad.shape)\n",
    "print('x test pad shape', xtest_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "f06e7445-a417-48e7-b936-69785fb7314a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 5000, 100)         2500000   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 50)                30200     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 255       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,533,005\n",
      "Trainable params: 2,533,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# vocab_size = 25000\n",
    "embedding_dim = 100\n",
    "pad_length =5000\n",
    "\n",
    "lstm_model = tf.keras.Sequential([\n",
    "    \n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=pad_length),\n",
    "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, dropout=0.2, recurrent_dropout=0.5)),\n",
    "    tf.keras.layers.LSTM(50, dropout=0.5),\n",
    "    tf.keras.layers.Dense(50, activation='relu'),\n",
    "    tf.keras.layers.Dense(5, activation='sigmoid')\n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "lstm_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'] )\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "ff5c00e5-bbb8-4790-8834-5255a678c083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "56/56 [==============================] - 365s 6s/step - loss: 1.5402 - accuracy: 0.3371 - val_loss: 1.3719 - val_accuracy: 0.4135\n",
      "Epoch 2/7\n",
      "56/56 [==============================] - 430s 8s/step - loss: 0.9999 - accuracy: 0.5601 - val_loss: 0.6728 - val_accuracy: 0.7640\n",
      "Epoch 3/7\n",
      "54/56 [===========================>..] - ETA: 17s - loss: 0.3382 - accuracy: 0.9132"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [193]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mlstm_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxtrain_pad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mytrain\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mxtest_pad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mytest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\binary\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\users\\binary\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1377\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1378\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1379\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1380\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1381\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1382\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1383\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1384\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1385\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1386\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\users\\binary\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\users\\binary\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\users\\binary\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\users\\binary\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2954\u001b[0m   (graph_function,\n\u001b[0;32m   2955\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\binary\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1849\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1851\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1852\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1853\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1854\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1855\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1856\u001b[0m     args,\n\u001b[0;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1858\u001b[0m     executing_eagerly)\n\u001b[0;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\users\\binary\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\users\\binary\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lstm_model.fit(xtrain_pad, ytrain , epochs=7, validation_data=(xtest_pad, ytest), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "7f0f261d-6d69-4701-9f83-828d2183d4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "bbc_vec = Word2Vec.load(\"bbc_new_word2vec_model_summary.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "65a16066-37a2-4557-8a06-7b9b6d00748a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('book', 0.9549551010131836),\n",
       " ('wife', 0.9448912143707275),\n",
       " ('colleague', 0.9417523741722107),\n",
       " ('husband', 0.935286283493042),\n",
       " ('Jane', 0.9320642948150635),\n",
       " ('career', 0.9318088889122009),\n",
       " ('Mrs', 0.9312762022018433),\n",
       " ('Ferguson', 0.92754065990448),\n",
       " ('Johnson', 0.9261930584907532),\n",
       " ('brief', 0.9258721470832825)]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc_vec.wv.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "bac3b7dc-1e66-4876-b30d-f6bd36af8527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 100)\n",
      "weighted matrix complete....\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab = model.wv.key_to_index\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "# vocab_size = len(w_index)\n",
    "# vocab_size = 25000\n",
    "dimension = 100\n",
    "weight_matrix = np.zeros((vocab_size, dimension))\n",
    "weight_matrix.shape\n",
    "\n",
    "\n",
    "vocab = tokenizer.word_index\n",
    "# total_vocab = 35000\n",
    "\n",
    "for word, index in tokenizer.word_index.items():\n",
    "#     print(word, (index-1))\n",
    "    if index < vocab_size: \n",
    "        if word in bbc_vec.wv.index_to_key:\n",
    "            weight_matrix[index] = bbc_vec.wv[word]\n",
    "#             print(len(weight_matrix[index-1]))\n",
    "        else: \n",
    "            weight_matrix[index] = np.zeros(100)\n",
    "\n",
    "print(weight_matrix.shape)\n",
    "print('weighted matrix complete....')\n",
    "weight_matrix[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "17880e2e-d856-46fd-8f34-edf16989f606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 5000, 100)         2500000   \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 100)               80400     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 5)                 505       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,591,005\n",
      "Trainable params: 2,591,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# lstm model using unsupervise word embedding \n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "\n",
    "# vocab_size = len(tokenizer.word_index)\n",
    "# vocab_size = 25000\n",
    "\n",
    "dimension = 100\n",
    "pad_length = max_val\n",
    "classes = 5\n",
    "\n",
    "wdnet_model = keras.Sequential([\n",
    "    keras.layers.Embedding(vocab_size, dimension, input_length=pad_length, weights=[weight_matrix], trainable=True),\n",
    "    tf.keras.layers.LSTM(100, dropout=0.5),\n",
    "    \n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(5, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "wdnet_model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    "\n",
    ")\n",
    "\n",
    "wdnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234b8422-d368-4c7a-ad06-c43cc0d2c6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wdnet_model.fit(xtrain_pad, ytrain , epochs=7, validation_data=(xtest_pad, ytest), verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
